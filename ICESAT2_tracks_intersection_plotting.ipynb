{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/ICESAT2/blob/main/ICESAT2_tracks_intersection_plotting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prerequisite\n"
      ],
      "metadata": {
        "id": "zXzj--gKOogA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[**Work Book**](docs.google.com/spreadsheets/d/1bYvEQ2ckp-xSrT_Xsa-lAnW3MEQU9K8AnXNjGL81C4g) **for processing links from earth data cloud ALT03 tracks in the region of ICESAT 2**"
      ],
      "metadata": {
        "id": "UftojGa1oItJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q20xiCSi-OTU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# list_B = [\n",
        "# \"https://n5eil02u.ecs.nsidc.org/esir/500000420137/22297319/processed_ATL03_20181027214446_04470106_005_01.h5\",\n",
        "# \"https://n5eil02u.ecs.nsidc.org/esir/500000420137/22319537/processed_ATL03_20181106091229_05920102_005_01.h5\",\n",
        "# \"https://n5eil02u.ecs.nsidc.org/esir/500000420137/22325051/processed_ATL03_20181125202044_08890106_005_01.h5\",\n",
        "# \"https://n5eil02u.ecs.nsidc.org/esir/500000420137/22343676/processed_ATL03_20181224185651_13310106_005_01.h5\",\n",
        "# \"https://n5eil02u.ecs.nsidc.org/esir/500000420137/22346036/processed_ATL03_20190107061605_01500202_005_01.h5\",\n",
        "# ]\n",
        "\n",
        "\n",
        "\n",
        "# for i in list_B:\n",
        "#   !wget --http-user=kroy0001 --http-password=/#j%kWrPA,8.HRe {i}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHoaOoaEy_qN"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/1kaiser/ICESAT2/releases/download/1/ASTER_DEM.tif #getting the dem\n",
        "!wget https://github.com/1kaiser/ICESAT2/releases/download/1/pongdam_dem.tif #getting Pong Dam dem\n",
        "!wget https://github.com/1kaiser/ICESAT2/releases/download/1/landsat2.zip\n",
        "!unzip '*.zip'\n",
        "!rm -r *.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas\n",
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "zA5D3GXo9lew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting tracks and solutions"
      ],
      "metadata": {
        "id": "b0kNPfe7uXzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Plotting all files in combination of Track 1 and Track 2**{ vertical-output: true }\n",
        "import glob\n",
        "import itertools\n",
        "import matplotlib.patheffects as path_effects\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "import numpy as np\n",
        "\n",
        "dem_in = rasterio.open('/content/pongdam_dem.tif')\n",
        "out_data = dem_in.read(1, masked=1)\n",
        "\n",
        "\n",
        "# Define the track names\n",
        "track_names = [\"gt1l\", \"gt1r\", \"gt2l\", \"gt2r\", \"gt3l\", \"gt3r\"]\n",
        "################################################################################\n",
        "def get_height_lat_lon(filename, track_name):\n",
        "  # Open the HDF5 file and extract data\n",
        "  f = h5py.File(filename, 'r')\n",
        "  h_ph_data = f['/'+str(track_name)+'/heights/h_ph']\n",
        "  lat_ph_data = f['/'+str(track_name)+'/heights/lat_ph']\n",
        "  lon_ph_data = f['/'+str(track_name)+'/heights/lon_ph']\n",
        "  return h_ph_data, lat_ph_data, lon_ph_data\n",
        "################################################################################\n",
        "def plot_track_combinations(file1, file2, axs):\n",
        "    # Loop through the track names and get the data\n",
        "    data = []\n",
        "    for i, tr in enumerate(track_names):\n",
        "        try:\n",
        "            h_ph_data1, lat1, lon1 = get_height_lat_lon(file1, tr)\n",
        "            data.append((lon1, lat1, h_ph_data1, f\"Track 1 {i+1} - {tr}\"))\n",
        "        except KeyError:\n",
        "            print(f\"Track {tr} not found in {file1}. Skipping...\")\n",
        "\n",
        "        try:\n",
        "            h_ph_data2, lat2, lon2 = get_height_lat_lon(file2, tr)\n",
        "            data.append((lon2, lat2, h_ph_data2, f\"Track 2 {i+7} - {tr}\"))\n",
        "        except KeyError:\n",
        "            print(f\"Track {tr} not found in {file2}. Skipping...\")\n",
        "\n",
        "    # Find the bounding box that covers all tracks with a buffer\n",
        "    min_lon, max_lon, min_lat, max_lat = float('inf'), float('-inf'), float('inf'), float('-inf')\n",
        "    for lon, lat, _, _ in data:\n",
        "        lon = np.array(lon)\n",
        "        lat = np.array(lat)\n",
        "        min_lon = min(np.min(lon), min_lon)\n",
        "        max_lon = max(np.max(lon), max_lon)\n",
        "        min_lat = min(np.min(lat), min_lat)\n",
        "        max_lat = max(np.max(lat), max_lat)\n",
        "\n",
        "    M_lon = (max_lon + min_lon)/2\n",
        "    M_lat = (max_lat + min_lat)/2\n",
        "    dx_boundary_x = max(max_lon - min_lon, max_lat - min_lat)\n",
        "\n",
        "    # Set the limits of the axis to the bounding box\n",
        "    axs.set_xlim(M_lon - dx_boundary_x/2, M_lon + dx_boundary_x/2)\n",
        "    axs.set_ylim(M_lat - dx_boundary_x/2, M_lat + dx_boundary_x/2)\n",
        "\n",
        "    show(dem_in, cmap='gist_ncar', ax=axs)\n",
        "\n",
        "    # Plot each track separately with a label in the legend and annotate the label\n",
        "    # Plot only the first point of each track with a label in the legend and annotate the label\n",
        "    for lon, lat, h_ph_data, label in data:\n",
        "        axs.scatter(lon, lat, s=1, marker='.', label=label)\n",
        "        # axs.annotate(label, (lon[len(lon)/2], lat[len(lat)/2]), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "        axs.annotate(label, (lon[len(lon)//2], lat[len(lat)//2]), xytext=(5, 5), textcoords='offset points', fontsize=8, bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
        "\n",
        "    # Add a legend to the plot\n",
        "    axs.legend(loc='lower right', scatterpoints=10)\n",
        "\n",
        "    # Add axis labels and a title\n",
        "    axs.set_xlabel('Longitude')\n",
        "    axs.set_ylabel('Latitude')\n",
        "    axs.set_title(f\"Tracks 1-6 from {file1} \\n and Tracks 7-12 from {file2}\")\n",
        "\n",
        "# Find all files with a .h5 extension in the current directory\n",
        "files = glob.glob(\"*.h5\")\n",
        "\n",
        "combinations = set()\n",
        "for f1, f2 in itertools.combinations(files, 2):\n",
        "    if (f2, f1) not in combinations:\n",
        "        combinations.add((f1, f2))\n",
        "\n",
        "# Determine the number of possible combinations and set the number of rows and columns\n",
        "num_combinations = len(list(combinations))\n",
        "nrows = int(num_combinations**0.5) + 1\n",
        "ncols = int(num_combinations/nrows) + 1\n",
        "\n",
        "# Create a figure with subplots for each combination\n",
        "fig, axs = plt.subplots(nrows, ncols, figsize=(100, 100))\n",
        "\n",
        "for i, (file1, file2) in enumerate(combinations):\n",
        "    row = i // ncols\n",
        "    col = i % ncols\n",
        "    index1 = row * nrows + col\n",
        "    index2 = col * nrows + row\n",
        "    print(f\"({row},{col}): {file1} {file2}\")\n",
        "    plot_track_combinations(file1, file2, axs[row, col])\n",
        "  "
      ],
      "metadata": {
        "id": "SDhwavSZayhA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "\n",
        "dem_in = rasterio.open('/content/pongdam_dem.tif')\n",
        "out_data = dem_in.read(1, masked=1)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#@title **tracks buffer plot with its intersections at centre as well as height vs lat/lon of the buffered region**{ vertical-output: true }\n",
        "fsize = 10\n",
        "filename_1 = \"/content/processed_ATL03_20200531180255_10110706_005_01.h5\" #@param {type:\"string\"}\n",
        "filename_2 = \"/content/processed_ATL03_20200211111433_07140602_005_01.h5\" #@param {type:\"string\"}\n",
        "tr_1 = \"gt1l\" #@param [\"gt1l\", \"gt1r\", \"gt2l\", \"gt2r\", \"gt3l\", \"gt3r\"]\n",
        "tr_2 = \"gt1r\" #@param [\"gt1l\", \"gt1r\", \"gt2l\", \"gt2r\", \"gt3l\", \"gt3r\"]\n",
        "\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import jax.numpy as jnp\n",
        "################################################################################\n",
        "# Calculate distance along track using haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "  R = 6371.0  # Earth radius in km\n",
        "  dLat = np.radians(lat2 - lat1)\n",
        "  dLon = np.radians(lon2 - lon1)\n",
        "  lat1 = np.radians(lat1)\n",
        "  lat2 = np.radians(lat2)\n",
        "  a = np.sin(dLat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dLon/2)**2\n",
        "  c = 2*np.arcsin(np.sqrt(a))\n",
        "  return R*c*1000.0\n",
        "################################################################################\n",
        "def get_height_lat_lon(filename, track_name):\n",
        "  # Open the HDF5 file and extract data\n",
        "  f = h5py.File(filename, 'r')\n",
        "  h_ph_data = f['/'+str(track_name)+'/heights/h_ph']\n",
        "  lat_ph_data = f['/'+str(track_name)+'/heights/lat_ph']\n",
        "  lon_ph_data = f['/'+str(track_name)+'/heights/lon_ph']\n",
        "  return h_ph_data, lat_ph_data, lon_ph_data\n",
        "################################################################################\n",
        "h_ph_data1, lat1, lon1 = get_height_lat_lon(filename_1, tr_1)\n",
        "h_ph_data2, lat2, lon2 = get_height_lat_lon(filename_2, tr_2)\n",
        "################################################################################\n",
        "# Create a new figure and axis\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "pos = ax.get_position()\n",
        "w_x, h_y = pos.width, pos.height\n",
        "################################################################################plotting complete dem and track\n",
        "lat_1x = np.array(lat1)\n",
        "lon_1x = np.array(lon1)\n",
        "lat_2x = np.array(lat2)\n",
        "lon_2x = np.array(lon2)\n",
        "\n",
        "# Find the bounding box that covers both tracks with a buffer\n",
        "min_lon = min(min(lon_1x), min(lon_2x)) - 0.001\n",
        "max_lon = max(max(lon_1x), max(lon_2x)) + 0.001\n",
        "min_lat = min(min(lat_1x), min(lat_2x)) - 0.001\n",
        "max_lat = max(max(lat_1x), max(lat_2x)) + 0.001\n",
        "M_lon = (max_lon + min_lon)/2\n",
        "M_lat = (max_lat + min_lat)/2\n",
        "dx_boundary_x = max(max_lon - min_lon, max_lat - min_lat)\n",
        "# Set the limits of the axis to the bounding box\n",
        "ax.set_xlim(M_lon - dx_boundary_x/2, M_lon + dx_boundary_x/2)\n",
        "ax.set_ylim(M_lat - dx_boundary_x/2, M_lat + dx_boundary_x/2)\n",
        "show(dem_in, cmap='gist_ncar', ax=ax)                            #plotting the dem over which the track points can be plotted\n",
        "# Add a grid to the plot\n",
        "ax.grid(True)\n",
        "\n",
        "# Add a legend to the plot\n",
        "# Add axis labels and a title\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.set_ylabel('Latitude')\n",
        "ax.set_title(f\"Tracks 1 '{filename_1}' '{tr_1}'\\n and \\nTracks 2 '{filename_2}' '{tr_2}'\")\n",
        "################################################################################\n",
        "lat_1 = np.array(lat1)\n",
        "lon_1 = np.array(lon1)\n",
        "lat_2 = np.array(lat2)\n",
        "lon_2 = np.array(lon2)\n",
        "################################################################################\n",
        "from shapely.geometry import Point\n",
        "from shapely.ops import unary_union\n",
        "from shapely.geometry import LineString\n",
        "\n",
        "# Convert the latitudes and longitudes to shapely Point objects\n",
        "points_1 = [Point(lon, lat) for lat, lon in zip(lat_1, lon_1)]\n",
        "points_2 = [Point(lon, lat) for lat, lon in zip(lat_2, lon_2)]\n",
        "\n",
        "# Find the intersection of the two tracks\n",
        "intersection = unary_union([LineString(points_1).intersection(LineString(points_2))])\n",
        "buffer_region_in_meters = 1000*4 #@param {type:\"raw\"} # in meters\n",
        "buffer_region = 1/(110 * 1000)* buffer_region_in_meters\n",
        "\n",
        "# Create a buffer around the intersection point\n",
        "intersection_buffer = intersection.buffer(buffer_region)\n",
        "#@title **track in the buffered region with altitude/elevatoion/height buffer control for points visibility with track labels** { vertical-output: true }\n",
        "# Compute the median height of the tracks within the buffered region\n",
        "import geopandas as gpd\n",
        "\n",
        "# Create a GeoDataFrame for the buffered intersection\n",
        "intersection_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries(intersection_buffer))\n",
        "\n",
        "# Create a mask for selecting points within the buffered region\n",
        "mask_1 = [intersection_buffer.contains(point) for point in points_1]\n",
        "mask_2 = [intersection_buffer.contains(point) for point in points_2]\n",
        "\n",
        "# Select the latitudes, longitudes, and heights within the buffered region for each track\n",
        "lat_1_buf = lat_1[mask_1]\n",
        "lon_1_buf = lon_1[mask_1]\n",
        "height_1_buf = np.array(h_ph_data1)[mask_1]\n",
        "\n",
        "lat_2_buf = lat_2[mask_2]\n",
        "lon_2_buf = lon_2[mask_2]\n",
        "height_2_buf = np.array(h_ph_data2)[mask_2]\n",
        "\n",
        "##########################################################\n",
        "# Set the limits of the axis to the bounding box\n",
        "# Find the bounding box that covers both tracks with a buffer\n",
        "min_lon = min(min(lon_1_buf), min(lon_2_buf)) - 0.001\n",
        "max_lon = max(max(lon_1_buf), max(lon_2_buf)) + 0.001\n",
        "min_lat = min(min(lat_1_buf), min(lat_2_buf)) - 0.001\n",
        "max_lat = max(max(lat_1_buf), max(lat_2_buf)) + 0.001\n",
        "M_lon = (max_lon + min_lon)/2\n",
        "M_lat = (max_lat + min_lat)/2\n",
        "dx_boundary = max(max_lon - min_lon, max_lat - min_lat)\n",
        "###########################################################\n",
        "################################################################################\n",
        "ax2 = fig.add_axes([0.5 -w_x/2 , 0-h_y , w_x , h_y])\n",
        "ax2.set_position([pos.x0 , 0-h_y, pos.width, h_y])\n",
        "#********************************\n",
        "pos = ax2.get_position()\n",
        "w_x, h_y = pos.width, pos.height\n",
        "#********************************\n",
        "\n",
        "x, y = intersection_buffer.exterior.xy\n",
        "\n",
        "ax.fill(x, y, alpha=0.6, fc='#fa19b6', ec='#fcc603', linestyle='--', linewidth=1)\n",
        "ax.scatter(lon_1x, lat_1x, s=1, color='blue', marker='.', label = 'Track 1')\n",
        "ax.scatter(lon_2x, lat_2x, s=1, color='r', marker='.', label = 'Track 2')\n",
        "ax.legend(loc='lower right', scatterpoints=10)\n",
        "\n",
        "show(dem_in, cmap='gist_ncar', ax=ax2)                    #showing the dem on which the tracks will be  plotted\n",
        "\n",
        "ax2.scatter(lon_1, lat_1,  marker='.', color='blue', s=1, label='Track 1')\n",
        "ax2.scatter(lon_2, lat_2,  marker='.', color='red', s=1, label='Track 2')\n",
        "ax2.fill(x, y, alpha=0.4, fc='#fa19b6', ec='#03fcc6', linestyle='dotted', linewidth=1)\n",
        "\n",
        "ax2.set_xlim(M_lon - dx_boundary/2, M_lon + dx_boundary/2)\n",
        "ax2.set_ylim(M_lat - dx_boundary/2, M_lat + dx_boundary/2)\n",
        "\n",
        "ax2.grid(True)\n",
        "ax2.set_xlabel('Longitude')\n",
        "ax2.set_ylabel('Latitude')\n",
        "ax2.set_title('Tracks 1 and 2 with Intersection Buffer')\n",
        "ax2.legend(['Track 1', 'Track 2'], loc='lower right', scatterpoints=10)\n",
        "################################################################################< elevation vs latitude\n",
        "median_height = np.median(np.concatenate((height_1_buf, height_2_buf)))\n",
        "buffer_height = 100 #@param {type:\"raw\"}\n",
        "\n",
        "ax3 = fig.add_axes([0.5 -w_x/2 , 0-h_y , w_x , h_y], sharey=ax2)\n",
        "ax3.set_position([pos.x0 + pos.width , pos.y0 , w_x/3, pos.height])\n",
        "\n",
        "ax3.scatter(height_1_buf, lat_1_buf,  marker='.', color='blue', s=1, label='Track 1')\n",
        "ax3.scatter(height_2_buf, lat_2_buf,  marker='.', color='red', s=1, label='Track 2')\n",
        "ax3.set_xlim(M_lat - dx_boundary/2, M_lat + dx_boundary/2)\n",
        "ax3.axvspan(median_height - buffer_height, median_height + buffer_height, color='#fdffa3', alpha=0.3)\n",
        "ax3.yaxis.set_label_position('right')\n",
        "ax3.yaxis.tick_right()\n",
        "ax3.set_ylabel('Y-axis label', visible=False)\n",
        "ax3.set_xlim([median_height - buffer_height, median_height + buffer_height])\n",
        "ax3.set_xlabel('Height')\n",
        "ax3.set_ylabel('Latitude')\n",
        "ax3.set_title('Height vs. Latitude for Tracks 1 and 2 within Buffered Region')\n",
        "ax3.grid(True,  color='black', alpha = 0.9)\n",
        "ax3.legend(loc='lower right', scatterpoints=10)\n",
        "################################################################################< elevation vs longitude\n",
        "ax4 = fig.add_axes([0 , 0 , w_x , h_y/3], sharex=ax2)\n",
        "ax4.set_position([pos.x0 , pos.y0 - h_y/3 -0.1, pos.width, h_y/3])\n",
        "\n",
        "ax4.scatter(lon_1_buf, height_1_buf,  marker='.', color='blue', s=1, label='Track 1')\n",
        "ax4.scatter(lon_2_buf, height_2_buf,  marker='.', color='red', s=1, label='Track 2')\n",
        "ax4.set_xlim(M_lon - dx_boundary/2, M_lon + dx_boundary/2)\n",
        "ax4.axhspan(median_height - buffer_height, median_height + buffer_height, color='#fdffa3', alpha=0.3)\n",
        "ax4.set_ylim([median_height - buffer_height, median_height + buffer_height])\n",
        "ax4.set_xlabel('Longitude')\n",
        "ax4.set_ylabel('Height')\n",
        "ax4.set_title('Height vs. Longitude for Tracks 1 and 2 within Buffered Region', loc='center').set_position([.5, 1.01])\n",
        "ax4.grid(True,  color='black', alpha = 0.9)\n",
        "ax4.legend(loc='lower right', scatterpoints=10)\n",
        "################################################################################< elevation vs track 1 & 2 distances\n",
        "ax5 = fig.add_axes([0.5 - w_x/2 , -h_y - h_y/3 -h_y/3 , w_x , h_y/3])\n",
        "ax5.set_position([pos.x0 ,-h_y - h_y/3 -h_y/3 - 0.1 * 2, pos.width, h_y/3])\n",
        "\n",
        "dist_ph_along_data_1 = [haversine(lat_1_buf[0], lon_1_buf[0], lat_1_buf[i], lon_1_buf[i]) for i in range(len(lat_1_buf))]\n",
        "\n",
        "lats = np.flip(lat_2_buf)\n",
        "longs = np.flip(lon_2_buf)\n",
        "heights = np.flip(height_2_buf)\n",
        "\n",
        "dist_ph_along_data_2 = [haversine(lats[0], longs[0], lats[i], longs[i]) for i in range(len(lats))]\n",
        "ax5.scatter(np.array(dist_ph_along_data_1), np.array(height_1_buf),  marker= '.', s=1, alpha= 0.4, label = \"Track 1\")\n",
        "ax5.scatter(np.array(dist_ph_along_data_2), np.array(heights),  marker= '.', s=1, alpha= 0.4, label = \"Track 2\")\n",
        "\n",
        "ax5.set_ylim([median_height - buffer_height, median_height + buffer_height])\n",
        "ax5.set_xlabel('Along-Track Distance(m)')\n",
        "ax5.set_ylabel('photon height')\n",
        "ax5.set_title('Zoomed-in view of h_ph dataset\\nfrom gt1l/heights', loc='center')\n",
        "ax5.grid(True,  color='black', alpha = 0.9)\n",
        "ax5.legend(loc='lower right', scatterpoints=10)\n",
        "\n",
        "# Show the plot\n",
        "plt.subplots_adjust(hspace=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6rSzMpeTUASq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing ellipsiod height to orthometruc height conversion"
      ],
      "metadata": {
        "id": "hkbfKfhClkdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transform"
      ],
      "metadata": {
        "id": "b1-g_DH3-zxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**once we get the `lat_1_buf`, `lon_1_buf`& `height_1_buf` then we can save it in *Point cloud* numpy format as `point_cloud.npy` for the purpose of point cloud classification and other things**"
      ],
      "metadata": {
        "id": "VxJLdAz1AGEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(np.array(lat_1_buf))"
      ],
      "metadata": {
        "id": "tIJXLWii_ZXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define your lat, lon, and height arrays\n",
        "lat_array = lat_1_buf\n",
        "lon_array = lon_1_buf\n",
        "height_array = height_1_buf\n",
        "\n",
        "# Combine the arrays into a single NumPy array\n",
        "point_cloud_array = np.column_stack((lon_array, lat_array, height_array))\n",
        "\n",
        "# Save the NumPy array to a file using np.save()\n",
        "np.save('point_cloud.npy', point_cloud_array)\n"
      ],
      "metadata": {
        "id": "2m63untV-14b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing graph cut"
      ],
      "metadata": {
        "id": "nZdamptWUgfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate 100 random points in a 2D plane\n",
        "points = np.random.rand(100, 2)\n",
        "\n",
        "# Compute Voronoi diagram\n",
        "vor = Voronoi(points)\n",
        "\n",
        "# Plot Voronoi diagram\n",
        "fig, ax = plt.subplots()\n",
        "voronoi_plot_2d(vor, ax=ax)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "upWq_0dE5Hdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define haversine function\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0  # Earth radius in km\n",
        "    dLat, dLon = np.radians(lat2 - lat1), np.radians(lon2 - lon1)\n",
        "    lat1, lat2 = np.radians(lat1), np.radians(lat2)\n",
        "    a = np.sin(dLat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dLon/2)**2\n",
        "    c = 2*np.arcsin(np.sqrt(a))\n",
        "    return R*c*1000.0\n",
        "\n",
        "# Load data from .npy file\n",
        "data = np.load('/content/point_cloud.npy')\n",
        "\n",
        "# Extract latitude, longitude, and elevation data\n",
        "lon, lat, elev = data[:,0], data[:,1], data[:,2]\n",
        "\n",
        "# Calculate linear distance between points\n",
        "dist = np.zeros_like(lat)\n",
        "for i in range(1, len(lat)):\n",
        "    dist[i] = dist[i-1] + haversine(lat[i-1], lon[i-1], lat[i], lon[i])\n",
        "\n",
        "# Create scatter plot\n",
        "fig, ax = plt.subplots(figsize=(10, 10), dpi = 100)\n",
        "ax.scatter(dist, elev, s= 1)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_xlabel('Linear Distance (m)')\n",
        "ax.set_ylabel('Elevation (m)')\n",
        "ax.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JCuznNWw9OMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "interval=100\n",
        "spacing=30\n",
        "ncols=10\n",
        "start_dist=0\n",
        "dist_intervals = []\n",
        "for i in range(len(dist)):\n",
        "    if dist[i] % interval == 0 and dist[i] >= start_dist:\n",
        "        dist_intervals.append((dist[i], dist[i] + interval))\n",
        "    elif dist[i] % interval >= spacing and dist[i] >= start_dist:\n",
        "        dist_intervals.append((dist[i], dist[i] + interval))\n",
        "num_intervals = len(dist_intervals)\n",
        "elev_intervals = [np.array([]) for i in range(num_intervals)]\n",
        "for i in range(num_intervals):\n",
        "    mask = (dist_intervals[i][0] <= dist) & (dist < dist_intervals[i][0] + interval) & (~np.isnan(elev))\n",
        "    elev_intervals[i] = elev[mask]\n",
        "    if (len(elev_intervals[i]) == 1):\n",
        "      elev_intervals[i] = elev_intervals[i - 1]\n",
        "\n",
        "def convert_data(dist_intervals, elev_intervals):\n",
        "  \"\"\"\n",
        "  Converts the data variable to take input as `(dist_intervals[:][0], elev_intervals[:][0])` distances and elevation arrays.\n",
        "\n",
        "  Args:\n",
        "    dist_intervals: The distances array.\n",
        "    elev_intervals: The elevation array.\n",
        "\n",
        "  Returns:\n",
        "    The converted data variable.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a copy of the data variable.\n",
        "  data = dist_intervals.copy()\n",
        "\n",
        "  # Loop over the distances array.\n",
        "  for i in range(len(dist_intervals)):\n",
        "    # Get the current distance interval.\n",
        "    current_dist_interval = dist_intervals[i]\n",
        "\n",
        "    # Get the current elevation interval.\n",
        "    current_elev_interval = elev_intervals[i]\n",
        "\n",
        "    # Set the value of the data variable at the current index to the product of the current distance interval and the current elevation interval.\n",
        "    data[i] = current_dist_interval * current_elev_interval\n",
        "\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "3daVy4Ac8dzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate some data with outliers.\n",
        "# data = elev_intervals[:][0]\n",
        "data = elev\n",
        "dist_a = dist\n",
        "print(len(data))\n",
        "#hampel filter\n",
        "def hampel_filter(data, window_size, k):\n",
        "  filtered_data = data.copy()\n",
        "  for i in range(len(data) - window_size + 1):\n",
        "    median = np.median(data[i:i + window_size])\n",
        "    mad = np.median(np.abs(data[i:i + window_size] - median))\n",
        "    threshold = k * mad\n",
        "    for j in range(window_size):\n",
        "      if np.abs(data[i + j] - median) > threshold:\n",
        "        filtered_data[i + j] = median\n",
        "\n",
        "  return filtered_data\n",
        "\n",
        "# Create scatter plot\n",
        "fig, ax = plt.subplots(figsize=(10, 5), dpi = 100)\n",
        "ax.scatter(dist, elev, s= 1)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_xlabel('Linear Distance (m)')\n",
        "ax.set_ylabel('Elevation (m)')\n",
        "ax.grid(True)\n",
        "# Create scatter plot\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5), dpi = 100)\n",
        "ax1.scatter(dist_a, data, s= 1)\n",
        "ax1.set_ylim(np.median(data)-10, np.median(data)+100)\n",
        "# ax1.set_aspect('equal')\n",
        "ax1.set_xlabel('Linear Distance (m)')\n",
        "ax1.set_ylabel('Elevation (m)')\n",
        "ax1.grid(True)\n",
        "# Filter the data using the Hampel filter with a window size of 5 and a threshold of 3.\n",
        "filtered_data = hampel_filter(data, 17, 3)\n",
        "print(len(filtered_data))\n",
        "\n",
        "from scipy.signal import medfilt\n",
        "# # Apply median filter with window size of 3 to filtered data\n",
        "median_filtered_data = medfilt(filtered_data, kernel_size=17)\n",
        "start = 0\n",
        "end = 40000 +start\n",
        "# Add a smooth moving average over the median filtered data.\n",
        "smooth_moving_average = np.convolve(median_filtered_data, np.ones(5)/5, mode='same')\n",
        "for i in range(0,4):\n",
        "  filtered_data = hampel_filter(median_filtered_data, 17, 3)\n",
        "  filtered_data = hampel_filter(filtered_data, 17, 3)\n",
        "  filtered_data = hampel_filter(filtered_data, 17, 3)\n",
        "  median_filtered_data = medfilt(filtered_data, kernel_size=17)\n",
        "  median_filtered_data = medfilt(median_filtered_data, kernel_size=17)\n",
        "\n",
        "smooth_moving_average = np.convolve(median_filtered_data, np.ones(5)/5, mode='same')\n",
        "\n",
        "# dist_a_avg = np.convolve(dist_a, np.ones(5)/5, mode='same')\n",
        "print(len(dist_a),len(smooth_moving_average), len(dist_a_avg))\n",
        "# Plot the original data, the Hampel filtered data, the median filtered data, and the smooth moving average.\n",
        "fig, ax = plt.subplots(figsize=(10, 5), dpi = 300)\n",
        "ax.scatter(dist_a[start:end], data[start:end], label=\"Original data\", s=1)\n",
        "ax.scatter(dist_a[start:end], filtered_data[start:end], label=\"Hampel filtered data\", s=1)\n",
        "ax.scatter(dist_a[start:end], median_filtered_data[start:end], label=\"Median filtered data\", s = 1)\n",
        "ax1.scatter(dist_a[start:end], median_filtered_data[start:end], label=\"Median filtered data\", s = 1, c = \"r\")\n",
        "ax.scatter(dist_a[start:end], smooth_moving_average[start:end], label=\"Smooth moving average\", s = 1)\n",
        "ax.set_ylim(np.median(median_filtered_data[start:end])-100, np.median(median_filtered_data[start:end])+100)\n",
        "\n",
        "# Add annotations for the beginning and end points.\n",
        "ax.annotate(f\"{smooth_moving_average[start]:.2f}\", xy=(dist_a_avg[start], smooth_moving_average[start]), xytext=(-20, 10), textcoords='offset points')\n",
        "ax.annotate(f\"{smooth_moving_average[end-1]:.2f}\", xy=(dist_a_avg[end-1], smooth_moving_average[end-1]), xytext=(-20, 10), textcoords='offset points')\n",
        "\n",
        "# ax.set_aspect('equal')\n",
        "ax.set_xlabel('Linear Distance (m)')\n",
        "ax.set_ylabel('Elevation (m)')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wB_AZ146iYhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = elev #elevation array\n",
        "dist_a = dist #linear distance array\n",
        "\n",
        "#hampel filter\n",
        "def hampel_filter(data, window_size, k):\n",
        "\n",
        "  # Create a copy of the data.\n",
        "  filtered_data = data.copy()\n",
        "\n",
        "  # Loop over the data, one window at a time.\n",
        "  for i in range(len(data) - window_size + 1):\n",
        "    # Calculate the median of the data within the window.\n",
        "    median = np.median(data[i:i + window_size])\n",
        "\n",
        "    # Calculate the MAD of the data within the window.\n",
        "    mad = np.median(np.abs(data[i:i + window_size] - median))\n",
        "\n",
        "    # Calculate the threshold for outliers.\n",
        "    threshold = k * mad\n",
        "\n",
        "    # If any of the data points in the window are outliers, replace them with the median.\n",
        "    for j in range(window_size):\n",
        "      if np.abs(data[i + j] - median) > threshold:\n",
        "        filtered_data[i + j] = median\n",
        "\n",
        "  return filtered_data\n",
        "\n",
        "\n",
        "# Filter the data using the Hampel filter with a window size of 5 and a threshold of 3.\n",
        "filtered_data = hampel_filter(data, 5, 30)\n",
        "print(len(filtered_data))\n",
        "\n",
        "from scipy.signal import medfilt\n",
        "# # Apply median filter with window size of 3 to filtered data\n",
        "median_filtered_data = medfilt(filtered_data, kernel_size=7)\n",
        "\n",
        "# Add a smooth moving average over the median filtered data.\n",
        "smooth_moving_average = np.convolve(median_filtered_data, np.ones(5)/5, mode='same')\n",
        "\n"
      ],
      "metadata": {
        "id": "P30Z8qF7qVVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def slope_detector(points, window_size, threshold):\n",
        "    slopes = []\n",
        "    for i in range(window_size, len(points)):\n",
        "        window_points = points[i-window_size:i]\n",
        "        x = np.arange(window_size)\n",
        "        y = window_points\n",
        "        m, b = np.polyfit(x, y, 1)  # find the slope of the line\n",
        "        slopes.append(m)\n",
        "    \n",
        "    # Find the points with slope greater than the threshold\n",
        "    idxs = np.where(np.abs(slopes) > threshold)[0]\n",
        "    differentiated_points = np.zeros_like(points)\n",
        "    differentiated_points[idxs + window_size//2] = points[idxs + window_size//2]\n",
        "    \n",
        "    return differentiated_points\n",
        "\n",
        "# Example usage\n",
        "points = smooth_moving_average[:]\n",
        "differentiated_points = slope_detector(points, window_size=17, threshold=0.04)\n",
        "plt.subplots(figsize=(10, 3), dpi = 100)\n",
        "plt.plot(points, 'b-', label='Original Points')\n",
        "plt.plot(differentiated_points, 'ro', label='Differentiated Points')\n",
        "# plt.gca().set_aspect('equal')\n",
        "plt.ylim(np.median(points)-10, np.median(points)+40)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jLNaIFVdRtUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def slope_detector(points, window_size, threshold):\n",
        "    slopes = []\n",
        "    for i in range(window_size, len(points)):\n",
        "        window_points = points[i-window_size:i]\n",
        "        x = np.arange(window_size)\n",
        "        y = window_points\n",
        "        m, b = np.polyfit(x, y, 1)  # find the slope of the line\n",
        "        slopes.append(m)\n",
        "    \n",
        "    # Find the points with slope greater than the threshold\n",
        "    idxs = np.where(np.abs(slopes) > threshold)[0]\n",
        "    inflection_points = np.column_stack((idxs + window_size//2, points[idxs + window_size//2]))\n",
        "    \n",
        "    return inflection_points\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "points = smooth_moving_average[:]\n",
        "window_size = 5\n",
        "threshold = 0.5\n",
        "\n",
        "inflection_points = slope_detector(points, window_size= 17, threshold= 0.04)\n",
        "plt.plot(points, 'b-', label='Original Points')\n",
        "\n",
        "plt.scatter(inflection_points[:, 0], inflection_points[:, 1])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IawtUIq73Ndl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_years"
      ],
      "metadata": {
        "id": "w6MSbxb8-I2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9S3e7FknC5Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "\n",
        "# Extract dates from list_C using regex\n",
        "dates = [datetime.datetime.strptime(re.search(r'processed_ATL03_(\\d{8})', path).group(1), \"%Y%m%d\").date() for path in list_C]\n",
        "\n",
        "# Convert dates to matplotlib dates\n",
        "mpl_dates = mdates.date2num(dates)\n",
        "\n",
        "# Extract unique years from dates\n",
        "unique_years = np.unique([date.year for date in dates])\n",
        "\n",
        "# Define the color map\n",
        "cmap = plt.cm.get_cmap('tab20')\n",
        "\n",
        "# Plot the data for each year\n",
        "fig, ax = plt.subplots(figsize=(4, 6))\n",
        "for i, year in enumerate(unique_years):\n",
        "  # Filter the dates and y-values to plot\n",
        "  mask = np.array([date.year == year for date in dates])\n",
        "  mpl_dates_filtered = mpl_dates[mask]\n",
        "  # Modify rand_y_values to contain days of the month\n",
        "  rand_x_values = [date.day for date in np.array(dates)[mask].tolist()]\n",
        "  # Assign a color to the year to plot\n",
        "  color = cmap(i/len(unique_years))\n",
        "\n",
        "  # Plot the data\n",
        "  ax.scatter(rand_x_values, mpl_dates_filtered, c=color, label=year)\n",
        "\n",
        "\n",
        "# Format the x-axis and y-axis\n",
        "ax.yaxis.set_major_locator(mdates.MonthLocator())\n",
        "ax.yaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
        "ax.tick_params(axis='y', labelrotation=0)\n",
        "ax.set_xlabel('X-Values')\n",
        "\n",
        "# Add a legend\n",
        "ax.legend(title='Years')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ukm9bvZG48kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "filename_1 = \"/content/processed_ATL03_20210131181032_05921002_005_01.h5\" #@param {type:\"string\"}\n",
        "tr_1 = \"gt2r\" #@param [\"gt1l\", \"gt1r\", \"gt2l\", \"gt2r\", \"gt3l\", \"gt3r\"]\n",
        "\n",
        "# Define haversine function\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0  # Earth radius in km\n",
        "    dLat, dLon = np.radians(lat2 - lat1), np.radians(lon2 - lon1)\n",
        "    lat1, lat2 = np.radians(lat1), np.radians(lat2)\n",
        "    a = np.sin(dLat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dLon/2)**2\n",
        "    c = 2*np.arcsin(np.sqrt(a))\n",
        "    return R*c*1000.0\n",
        "def get_height_lat_lon(filename, track_name):\n",
        "  # Open the HDF5 file and extract data\n",
        "  f = h5py.File(filename, 'r')\n",
        "  h_ph_data = f['/'+str(track_name)+'/heights/h_ph']\n",
        "  lat_ph_data = f['/'+str(track_name)+'/heights/lat_ph']\n",
        "  lon_ph_data = f['/'+str(track_name)+'/heights/lon_ph']\n",
        "  return h_ph_data, lat_ph_data, lon_ph_data\n",
        "  \n",
        "\n",
        "h_ph_data1, lat1, lon1 = get_height_lat_lon(filename_1, tr_1)\n",
        "# Combine the arrays into a single NumPy array\n",
        "point_cloud_array = np.column_stack((lon1, lat1, h_ph_data1))\n",
        "np.save('point_cloud.npy', point_cloud_array)\n",
        "\n"
      ],
      "metadata": {
        "id": "PsOSINv9atfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from .npy file\n",
        "data = np.load('/content/point_cloud.npy')\n",
        "\n",
        "# Extract latitude, longitude, and elevation data\n",
        "lon, lat, elev = data[:,0], data[:,1], data[:,2]"
      ],
      "metadata": {
        "id": "aU9lKDR2Co02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load data from .npy file\n",
        "data = np.load('/content/point_cloud.npy')\n",
        "\n",
        "# Extract latitude, longitude, and elevation data\n",
        "lon, lat, elev = data[:,0], data[:,1], data[:,2]\n",
        "\n",
        "# Calculate linear distance between points\n",
        "dist = np.zeros_like(lat)\n",
        "for i in range(1, len(lat)):\n",
        "    dist[i] = dist[i-1] + haversine(lat[i-1], lon[i-1], lat[i], lon[i])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate some data with outliers.\n",
        "# data = elev_intervals[:][0]\n",
        "data = elev\n",
        "dist_a = dist\n",
        "print(len(data))\n",
        "#hampel filter\n",
        "def hampel_filter(data, window_size, k):\n",
        "  filtered_data = data.copy()\n",
        "  for i in range(len(data) - window_size + 1):\n",
        "    median = np.median(data[i:i + window_size])\n",
        "    mad = np.median(np.abs(data[i:i + window_size] - median))\n",
        "    threshold = k * mad\n",
        "    for j in range(window_size):\n",
        "      if np.abs(data[i + j] - median) > threshold:\n",
        "        filtered_data[i + j] = median\n",
        "\n",
        "  return filtered_data\n",
        "\n",
        "# Create scatter plot\n",
        "fig, ax = plt.subplots(figsize=(10, 5), dpi = 100)\n",
        "ax.scatter(dist, elev, s= 1)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_xlabel('Linear Distance (m)')\n",
        "ax.set_ylabel('Elevation (m)')\n",
        "ax.grid(True)\n",
        "# Create scatter plot\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5), dpi = 100)\n",
        "ax1.scatter(dist_a, data, s= 1)\n",
        "ax1.set_ylim(np.median(data)-10, np.median(data)+100)\n",
        "# ax1.set_aspect('equal')\n",
        "ax1.set_xlabel('Linear Distance (m)')\n",
        "ax1.set_ylabel('Elevation (m)')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Filter the data using the Hampel filter with a window size of 5 and a threshold of 3.\n",
        "filtered_data = hampel_filter(data, 17, 3)\n",
        "print(len(filtered_data))\n",
        "\n",
        "from scipy.signal import medfilt\n",
        "# # Apply median filter with window size of 3 to filtered data\n",
        "median_filtered_data = medfilt(filtered_data, kernel_size=17)\n",
        "start = 0\n",
        "end = 2000 +start\n",
        "# Add a smooth moving average over the median filtered data.\n",
        "smooth_moving_average = np.convolve(median_filtered_data, np.ones(5)/5, mode='same')\n",
        "\n",
        "# Plot the original data, the Hampel filtered data, the median filtered data, and the smooth moving average.\n",
        "fig, ax = plt.subplots(figsize=(10, 5), dpi = 300)\n",
        "ax.scatter(dist_a[start:end], data[start:end], label=\"Original data\", s=1)\n",
        "ax.scatter(dist_a[start:end], filtered_data[start:end], label=\"Hampel filtered data\", s=1)\n",
        "ax.scatter(dist_a[start:end], median_filtered_data[start:end], label=\"Median filtered data\", s = 1)\n",
        "ax1.scatter(dist_a[start:end], median_filtered_data[start:end], label=\"Median filtered data\", s = 1, c = \"r\")\n",
        "ax.scatter(dist_a[start:end], smooth_moving_average[start:end], label=\"Smooth moving average\", s = 1)\n",
        "ax.set_ylim(np.median(median_filtered_data[start:end])-100, np.median(median_filtered_data[start:end])+100)\n",
        "\n",
        "# # Add annotations for the beginning and end points.\n",
        "# ax.annotate(f\"{smooth_moving_average[start]:.2f}\", xy=(dist_a_avg[start], smooth_moving_average[start]), xytext=(-20, 10), textcoords='offset points')\n",
        "# ax.annotate(f\"{smooth_moving_average[end-1]:.2f}\", xy=(dist_a_avg[end-1], smooth_moving_average[end-1]), xytext=(-20, 10), textcoords='offset points')\n",
        "\n",
        "# ax.set_aspect('equal')\n",
        "ax.set_xlabel('Linear Distance (m)')\n",
        "ax.set_ylabel('Elevation (m)')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0GF4r6nQIFDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preparation for **plotting in river buffer**"
      ],
      "metadata": {
        "id": "d3MqLvFb9Ztk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading river buffer\n",
        "!wget https://github.com/1kaiser/ICESAT2/releases/download/1/parvati_catchment.zip\n",
        "!unzip parvati_catchment.zip -d parvati_catchment\n",
        "#downloading catchment\n",
        "!wget https://github.com/1kaiser/ICESAT2/releases/download/1/parvati_river_buffer.zip\n",
        "!unzip parvati_river_buffer.zip -d parvati_river_buffer\n",
        "#parallel downloading of ICESAT 2 tracks\n",
        "!wget https://github.com/1kaiser/ICESAT2/releases/download/1/parvati_tracks.txt\n",
        "import multiprocessing as mp\n",
        "import subprocess\n",
        "import re\n",
        "# Define the regex pattern to match the links\n",
        "pattern = r\"https://n5eil02u.ecs.nsidc.org/esir/.*?\\.h5\"\n",
        "with open(\"parvati_tracks.txt\", \"r\") as file:\n",
        "    contents = file.read()\n",
        "list_C = re.findall(pattern, contents)\n",
        "\n",
        "# Define the function to download a file using wget\n",
        "def download_file(url):\n",
        "    subprocess.Popen(['wget', '--http-user=kroy0001', '--http-password=/#j%kWrPA,8.HRe', url]).wait()\n",
        "\n",
        "# Create a pool of processes to download the files\n",
        "num_processes = mp.cpu_count() * 2\n",
        "pool = mp.Pool(processes=num_processes)\n",
        "results = [pool.apply_async(download_file, args=(url,)) for url in list_C]\n",
        "\n",
        "# Wait for all the processes to finish\n",
        "for r in results:\n",
        "    r.wait()"
      ],
      "metadata": {
        "id": "1I-V9d0KyvEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**funny day**"
      ],
      "metadata": {
        "id": "A_MvTe54P6mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas"
      ],
      "metadata": {
        "id": "G9BaGKAXP-gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title filing { vertical-output: true }\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import datetime\n",
        "from shapely.geometry import Point\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the vector file\n",
        "vector_file = \"/content/parvati_river_buffer/Export_Output.shp\"\n",
        "region = gpd.read_file(vector_file)\n",
        "\n",
        "def get_height_lat_lon(filename, track_name):\n",
        "  # Open the HDF5 file and extract data\n",
        "  f = h5py.File(filename, 'r')\n",
        "  h_ph_data = f['/'+str(track_name)+'/heights/h_ph']\n",
        "  lat_ph_data = f['/'+str(track_name)+'/heights/lat_ph']\n",
        "  lon_ph_data = f['/'+str(track_name)+'/heights/lon_ph']\n",
        "  file_name = np.array([filename] * len(h_ph_data))\n",
        "  track_name = np.array([track_name] * len(h_ph_data))\n",
        "  return np.column_stack((file_name, track_name, lon_ph_data, lat_ph_data, h_ph_data))\n",
        "\n",
        "# Define the track names to be checked\n",
        "track_names = [\"gt1l\", \"gt1r\", \"gt2l\", \"gt2r\", \"gt3l\", \"gt3r\"]\n",
        "\n",
        "h5_files = glob.glob(\"*.h5\")\n",
        "file_list = []\n",
        "\n",
        "for filename in h5_files:\n",
        "  try:\n",
        "    f = h5py.File(filename, 'r')\n",
        "    for track_name in track_names:\n",
        "      np.array(f['/'+str(track_name)+'/heights/h_ph'])\n",
        "      file_list.append((filename, track_name)) # Append (filename, track_name) tuple to file_list\n",
        "  except KeyError:\n",
        "    count = 0\n",
        "\n",
        "clipped_points_array = []\n",
        "\n",
        "h5_files = file_list\n",
        "count = 0\n",
        "def process_data_by_year(h5_files, year, region):\n",
        "    for filename, track in tqdm(h5_files):\n",
        "        date_string = re.search(r'processed_ATL03_(\\d{8})', filename).group(1)\n",
        "        date_object = datetime.datetime.strptime(date_string, \"%Y%m%d\").date().year\n",
        "        if date_object == year:\n",
        "            try:\n",
        "                track_data = get_height_lat_lon(filename, track)\n",
        "                np.save(f'point_cloud_{filename[:-3]}_{track}.npy', track_data)\n",
        "                point_cloud_file = f'point_cloud_{filename[:-3]}_{track}.npy'\n",
        "                point_cloud = np.load(point_cloud_file, allow_pickle=True)\n",
        "                file_names, track_names, lon, lat, h_ph_data = point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2], point_cloud[:, 3], point_cloud[:, 4]\n",
        "                geometry = [Point(lon[i], lat[i]) for i in range(len(lon))]\n",
        "                data = gpd.GeoDataFrame({'file_name': file_names, 'track_name': track_names,'h_ph_data': h_ph_data, 'lat': lat, 'lon': lon},geometry=geometry, crs=region.crs)\n",
        "                clipped_points = gpd.sjoin(data, region, op='within')\n",
        "                clipped_points_array.append(np.column_stack((clipped_points['file_name'], clipped_points['track_name'], clipped_points['h_ph_data'], clipped_points['lat'], clipped_points['lon'])))\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                region.plot(ax=ax, color='white', edgecolor='black')\n",
        "                data.plot(ax=ax, column='h_ph_data', markersize=1)\n",
        "\n",
        "                plt.title(\"Points inside the Buffered Vector\")\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "            except KeyError:\n",
        "                count += 1\n",
        "year_list = [2018, 2019, 2020, 2021, 2022]\n",
        "for x in year_list:\n",
        "  process_data_by_year(h5_files, x, region)\n",
        "\n",
        "np.save('clipped_points.npy', clipped_points_array)\n"
      ],
      "metadata": {
        "id": "f-Yg431cP-gM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **plotting functions**\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import medfilt\n",
        "\n",
        "# Define haversine function\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0  # Earth radius in km\n",
        "    dLat, dLon = np.radians(lat2 - lat1), np.radians(lon2 - lon1)\n",
        "    lat1, lat2 = np.radians(lat1), np.radians(lat2)\n",
        "    a = np.sin(dLat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dLon/2)**2\n",
        "    c = 2*np.arcsin(np.sqrt(a))\n",
        "    return R*c*1000.0\n",
        "\n",
        "\n",
        "def elevation_plot(data, window_size=17, k=3):\n",
        "    # Load data from .npy file\n",
        "    # data = np.load(np_file, allow_pickle=True)\n",
        "\n",
        "    # Extract latitude, longitude, and elevation data\n",
        "    lon, lat, elev = data[:,4].astype(float), data[:,3].astype(float), data[:,2].astype(float)\n",
        "    # Calculate linear distance between points\n",
        "    dist = np.zeros_like(lat)\n",
        "    for i in range(1, len(lat)):\n",
        "        dist[i] = dist[i-1] + haversine(lat[i-1], lon[i-1], lat[i], lon[i])\n",
        "\n",
        "    # Apply Hampel filter with specified window size and threshold\n",
        "    filtered_data = hampel_filter(elev, window_size, k)\n",
        "\n",
        "    # Apply median filter with window size of 17 to filtered data\n",
        "    median_filtered_data = medfilt(filtered_data, kernel_size=17)\n",
        "\n",
        "    # Add a smooth moving average over the median filtered data.\n",
        "    smooth_moving_average = np.convolve(median_filtered_data, np.ones(5)/5, mode='same')\n",
        "\n",
        "    # for i in range(0,10):\n",
        "    #   filtered_data = hampel_filter(smooth_moving_average, window_size, k)\n",
        "    #   median_filtered_data = medfilt(filtered_data, kernel_size=17)\n",
        "    #   smooth_moving_average = np.convolve(median_filtered_data, np.ones(5)/5, mode='same')\n",
        "\n",
        "    # Create scatter plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 5), dpi=300)\n",
        "    ax.scatter(dist, elev, label=\"Original data\", s=1)\n",
        "    ax.scatter(dist, filtered_data, label=\"Hampel filtered data\", s=1)\n",
        "    ax.scatter(dist, median_filtered_data, label=\"Median filtered data\", s=1)\n",
        "    ax.scatter(dist, smooth_moving_average, label=\"Smooth moving average\", s=1, c='r')\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('Linear Distance (m)')\n",
        "    ax.set_ylabel('Elevation (m)')\n",
        "    ax.set_title('Elevation Profile '+data[0,0]+' '+data[0,1])\n",
        "    ax.legend()\n",
        "\n",
        "# Hampel filter\n",
        "def hampel_filter(data, window_size, k):\n",
        "    filtered_data = data.copy()\n",
        "    for i in range(len(data) - window_size + 1):\n",
        "        median = np.median(data[i:i + window_size])\n",
        "        mad = np.median(np.abs(data[i:i + window_size] - median))\n",
        "        threshold = k * mad\n",
        "        for j in range(window_size):\n",
        "            if np.abs(data[i + j] - median) > threshold:\n",
        "                filtered_data[i + j] = median\n",
        "\n",
        "    return filtered_data\n"
      ],
      "metadata": {
        "id": "nmlUt0teDQJU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point_cloud_file = f'/content/clipped_points.npy'\n",
        "P_C = np.load(point_cloud_file, allow_pickle=True)\n",
        "PC_list =[]\n",
        "for point_cloud in P_C:\n",
        "  if len(point_cloud) != 0:\n",
        "    PC_list.append(point_cloud)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "region.plot(ax=ax, color='white', edgecolor='black')\n",
        "\n",
        "for point_cloud in PC_list:\n",
        "  # Extract the file names, track names, and point cloud data from the numpy array\n",
        "  file_names = point_cloud[:, 0]\n",
        "  track_names = point_cloud[:, 1]  \n",
        "  h_ph_data = point_cloud[:, 2]\n",
        "  lat = point_cloud[:, 3]\n",
        "  lon = point_cloud[:, 4]\n",
        "\n",
        "  # Create a GeoDataFrame for the point cloud data\n",
        "  geometry = [Point(lon[i], lat[i]) for i in range(len(lon))]\n",
        "  data = gpd.GeoDataFrame({'file_name': file_names, 'track_name': track_names,'h_ph_data': h_ph_data, 'lat': lat, 'lon': lon}, geometry=geometry, crs=region.crs)\n",
        "\n",
        "  # Spatially join the points with the region to get the clipped points\n",
        "  clipped_points = gpd.sjoin(data, region, op='within')\n",
        "  clipped_points.plot(ax=ax, column='h_ph_data', markersize=1)\n",
        "\n",
        "  first_points = clipped_points.groupby('track_name').first()\n",
        "  for idx, row in first_points.iterrows():\n",
        "    point = row['geometry']\n",
        "    label = row['file_name']\n",
        "    ax.annotate(label, xy=(point.x, point.y), xytext=(3, 3), textcoords=\"offset points\", fontsize=8, color='red')\n",
        "    \n",
        "  elevation_plot(point_cloud, window_size=17, k=3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1njBn1CGwOIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}